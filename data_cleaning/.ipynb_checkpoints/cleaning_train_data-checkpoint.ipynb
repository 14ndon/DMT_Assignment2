{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Configure visualizations\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "df = pd.read_csv('../data/training_set_VU_DM.csv')\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.head()\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visitor_hist_starrating      4706481\n",
      "visitor_hist_adr_usd         4705359\n",
      "prop_review_score               7364\n",
      "prop_location_score2         1090348\n",
      "srch_query_affinity_score    4640941\n",
      "orig_destination_distance    1607782\n",
      "comp1_rate                   4838417\n",
      "comp1_inv                    4828788\n",
      "comp1_rate_percent_diff      4863908\n",
      "comp2_rate                   2933675\n",
      "comp2_inv                    2828078\n",
      "comp2_rate_percent_diff      4402109\n",
      "comp3_rate                   3424059\n",
      "comp3_inv                    3307357\n",
      "comp3_rate_percent_diff      4485550\n",
      "comp4_rate                   4650969\n",
      "comp4_inv                    4614684\n",
      "comp4_rate_percent_diff      4827261\n",
      "comp5_rate                   2735974\n",
      "comp5_inv                    2598327\n",
      "comp5_rate_percent_diff      4117248\n",
      "comp6_rate                   4718190\n",
      "comp6_inv                    4697371\n",
      "comp6_rate_percent_diff      4862173\n",
      "comp7_rate                   4642999\n",
      "comp7_inv                    4601925\n",
      "comp7_rate_percent_diff      4819832\n",
      "comp8_rate                   3041693\n",
      "comp8_inv                    2970844\n",
      "comp8_rate_percent_diff      4343617\n",
      "gross_bookings_usd           4819957\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data in the entire dataframe\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Show columns with missing data\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set all missing competitor fields to 0\n",
    "comp_cols = [col for col in df.columns if col.startswith('comp')]\n",
    "df[comp_cols] = df[comp_cols].fillna(0)\n",
    "\n",
    "# 2. Impute 'orig_destination_distance' and 'prop_review_score' with their median\n",
    "for col in ['orig_destination_distance', 'prop_review_score']:\n",
    "    median_val = df[col].median()\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# 3. Impute 'srch_query_affinity_score' with the minimum value\n",
    "df['srch_query_affinity_score'] = df['srch_query_affinity_score'].fillna(df['srch_query_affinity_score'].min())\n",
    "\n",
    "# 4. Impute 'prop_location_score2' with the minimum per 'srch_destination_id'\n",
    "df['prop_location_score2'] = df.groupby('srch_destination_id')['prop_location_score2'].transform(\n",
    "    lambda x: x.fillna(x.min())\n",
    ")\n",
    "df['prop_location_score2'] = df['prop_location_score2'].fillna(df['prop_location_score2'].median())\n",
    "\n",
    "\n",
    "# 5. Impute visitor historical features with mean\n",
    "visitor_hist_cols = ['visitor_hist_starrating', 'visitor_hist_adr_usd']\n",
    "for col in visitor_hist_cols:\n",
    "    mean_val = df[col].mean()\n",
    "    df[col] = df[col].fillna(mean_val)\n",
    "\n",
    "# 6. Normalize 'price_usd' and 'prop_starrating' based on search and property groups\n",
    "\n",
    "# Normalize 'price_usd' by 'srch_id'\n",
    "df['price_usd_norm'] = df['price_usd'] / df.groupby('srch_id')['price_usd'].transform('mean')\n",
    "\n",
    "# Normalize 'prop_starrating' by 'prop_id'\n",
    "df['prop_starrating_norm'] = df['prop_starrating'] / df.groupby('prop_id')['prop_starrating'].transform('mean')\n",
    "df['prop_starrating_norm'] = df['prop_starrating_norm'].fillna(0)\n",
    "\n",
    "# 7. Drop the original 'price_usd' feature\n",
    "df = df.drop(columns=['price_usd'])\n",
    "\n",
    "# 8. Remove features not needed\n",
    "# cols_to_drop = ['gross_bookings_usd', 'site_id', 'prop_id', 'srch_id']\n",
    "cols_to_drop = ['gross_bookings_usd', 'position']\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/clean_train.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_percentages = df['click_bool'].value_counts(normalize=True) * 100\n",
    "print(click_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate the two classes\n",
    "clicked = df[df['click_bool'] == 1]\n",
    "not_clicked = df[df['click_bool'] == 0]\n",
    "\n",
    "# 2. Downsample the majority class (not clicked)\n",
    "not_clicked_downsampled = not_clicked.sample(n=len(clicked), random_state=42)\n",
    "\n",
    "# 3. Combine the two classes back together\n",
    "df_balanced = pd.concat([clicked, not_clicked_downsampled])\n",
    "\n",
    "# 4. Shuffle the resulting dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count original numbers\n",
    "n_clicked = len(clicked)\n",
    "n_not_clicked = len(not_clicked)\n",
    "\n",
    "# Calculate how many will be dropped\n",
    "rows_dropped = n_not_clicked - n_clicked\n",
    "\n",
    "print(f\"Rows with click_bool = 1: {n_clicked}\")\n",
    "print(f\"Rows with click_bool = 0 before downsampling: {n_not_clicked}\")\n",
    "print(f\"Rows to be dropped: {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_percentages = df_balanced['click_bool'].value_counts(normalize=True) * 100\n",
    "print(click_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_counts = df_balanced['click_bool'].value_counts()\n",
    "print(click_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
